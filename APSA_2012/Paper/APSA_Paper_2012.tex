% EBMA paper for Political Analysis: As Accepted for Publication.



%\documentclass[pdftex,12pt,fullpage,oneside,endnotes]{amsart}
\documentclass[12pt,fullpage,endnotes]{article}
%\usepackage{apsr}
\usepackage{array,amsmath,psfrag,amssymb,subfigure,tabularx}
\usepackage{multicol}
\usepackage{booktabs}
\usepackage[usenames]{color}
\usepackage{datetime}
\usepackage{dcolumn}
\usepackage{wrapfig}
\usepackage{setspace}
\usepackage{url}
\usepackage[english]{babel}
\usepackage{times}
\usepackage{multirow}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf}
\usepackage{lscape}
\usepackage{array}
\usepackage{booktabs}
%\usepackage{endnotes}
\usepackage{rotating}
\usepackage[top=2.54cm, bottom=2.54cm, left=2.54cm, right=2.54cm]{geometry} 
%\usepackage[nolist]{endfloat}

\newcommand{\note}[1]{\footnote{ #1 \vspace{4 mm}}}

\usepackage{natbib}
\bibpunct{(}{)}{;}{a}{}{,}
\bibdata{Bibliography_EBMA}
%\bibliographystyle{chicago}

\newboolean{blind}
\setboolean{blind}{false}


\title{Say Yes to the Guess: \\ Tailoring Elegant Ensembles on a Tight
  (Data) Budget\thanks{Prepared for the 2012 Annual Meeting of the American Political Science Association, August 30 - September 2, New Orleans, Louisiana. 
This work was partially supported by the Information Processing Technology Office of the Defense Advanced Research Projects Agency via a holding grant to the Lockheed Martin Corporation, Contract FA8650-07-C-7749. The current support is partially from the Office of Naval Research via ONR contract N00014-12-C-0066 to Lockheed Martin's Advanced Technology Laboratories.
    }}
\author{
Jacob M. Montgomery\\
	Department of Political Science\\
	Washington University in St. Louis\\
	Campus Box 1063, One Brookings Drive\\
	St. Louis, MO, USA, 63130-4899 
	\and
Florian M. Hollenbach  \\
	Department of Political Science\\
	Duke University\\
	Perkins Hall 326 Box 90204\\
	Durham, NC, USA, 27707-4330
	\and
Michael D. Ward\\
	Department of Political Science\\
	Duke University\\
	Perkins Hall 326 Box 90204\\
	Durham, NC, USA, 27707-4330\\
	corresponding author: michael.d.ward@duke.edu
} 




\date{\today}


\begin{document}

\maketitle
\thispagestyle{empty}
\clearpage
\pagestyle{myheadings}
\markright{Montgomery, Hollenbach, \& Ward\hfill Ensemble BMA\hfill}
\newpage
\singlespacing

\thispagestyle{empty}

{\centering \bf \large Say Yes to the Guess: \\ Tailoring Elegant Ensembles on a Tight (Data) Budget \\}


\begin{center}
\begin{tabular}{c@{ }c@{ }c}

Jacob M. Montgomery, & Florian M. Hollenbach, & and Michael D. Ward\\
\end{tabular}
\end{center}

\begin{abstract}
  \noindent We consider ensemble Bayesian model averaging (EBMA) in
  the context of small-n prediction tasks in the presence of a large
  number of component models.  With a large number of observations to
  calibrate ensembles, relatively small numbers of component
  forecasts, and low rates of missingness, the standard approach to
  calibrating forecasting ensembles introduced by \cite{Raftery:2005}
  performs well. However, data in the social sciences generally do not
  fulfill these requirements. The number of outcomes predicted tends
  to be small, the number of forecasting models in the literature can
  be large, and missing predictions for component models are neither
  random nor rare. In these circumstances, EBMA models may miss-weight
  components, undermining the advantages of the ensemble approach to
  prediction.  In this article, we explore these issues in the context
  of ensemble prediction and provide a solution that diminishes
  undesirable outcomes by introducing a ``wisdom of the crowds''
  parameter to the standard EBMA framework. We show that this solution
  improves predictive accuracy of EBMA forecasts in both political and
  economic applications.
\end{abstract}

\doublespacing
%\newpage


\setcounter{page}{1}

\section{Introduction}

Although accurate prediction of future events is not the primary goal
for most social sciences, recent years have witnessed spreading of
systematic forecasting from more traditional topics such as GDP growth
and unemployment to many new domains including elections
\citep[e.g.,][]{Linzer:2013}, political instability
\citep[e.g.,][]{Goldstone:etal:2010}, and mass killings
\citep{Ulfelder:2012}. Several factors motivate this trend. To begin
with, testing predictions about future events against observed
outcomes is seen as a stringent validity check of statistical and
theoretical models. In addition, forecasting of important political,
economic, and social events is of great interest to policymakers and
the public.


With the proliferation of forecasting efforts, however, comes a need
for sensible methods to aggregate and utilize the various scholarly
efforts. One attractive solutions to this problem is to combine
prediction models and create an ensemble forecast. Combining
forecasts reduces reliance on any single data source or methodology,
and allows for the incorporation of more information than any one model can provide in isolation. Across subject domains, scholars have shown
ensemble predictions to be more accurate than any individual component
model and less likely to make dramatically incorrect predictions
\citep{Bates:1969,Armstrong:2001,Raftery:2005}.

One promising approach to combining multiple forecasts is ensemble
Bayesian model averaging (EBMA). This method was first proposed by
\citet{Raftery:2005} to combine weather forecasts and was introduced
to the social sciences by \citet{mhw:2012}. EBMA combines multiple
forecasts using a finite mixture model that generates a weighted
predictive probability density function (PDF). EBMA mixture models
seek to collate the good parts of existing forecasting models, while
avoiding over-fitting to past observations and over-estimating our
certainty about forecasts of future events. The hope is for greater accuracy as both the knowledge and implied uncertainty of a variety of approaches are
integrated into a combined predictive PDF.

In this article, we present several adjustments to the basic EBMA
model as specified in \citet{mhw:2012} that can aid applied
researchers to create ensemble forecasts in the presence of
data-quality challenges common in real-world social science settings.
Specifically, we show EBMA can be adjusted to accommodate small
calibration samples, large numbers of candidate components, and
missing forecasts.  We propose an alteration to the basic model to
hedge against the miss-weighting of components resulting from either
strong or poor performance in the limited calibration period.  After
discussing the data-quality challenges commonly experienced in
ensemble forecasting, we introduce the basic EBMA model and outline
modifications to the model for small samples and missing components in
Section \ref{model}. In Section \ref{empirics}, we demonstrate how our
adjustment to the basic EBMA model improves out-of-sample forecasts in
a simulation study and apply the method to predict the
U.S. unemployment rate and the 2012 U.S. presidential election.


\section{Ensemble prediction with sparse data and multiple forecasts }
\label{theproblem}

The concept of ensemble forecasting builds on the basic notion that
combining multiple points of view leads to a more accurate picture of
reality \citep[c.f.,][]{Surowiecki:2004}.  Among the more famous
demonstrations of this phenomenon is a competition to guess the weight
of an ox at the West Of England Fat Stock and Poultry Exhibition
reported by \citet{Galton:1907}.  Galton famously demonstrated that,
while individual entrants were often wildly inaccurate, aggregating
the ``wisdom of crowds'' by using the average guess resulted in a
remarkably accurate estimate.  This observation is related to
Condorcet's jury theorem that calculates the probability of a jury
reaching a correct verdict, based on independent juror opinions, will always be greater than even the most accurate
of judges.

In recent years, the advantages of ensembles have come to play a
particularly prominent role in the machine learning and nonparametric
statistics community \citep{Hastie:2009}. A wide range of approaches
including neural nets, additive regression trees, and K nearest
neighbors fall under the general umbrella of ensemble approaches.  Of
particular relevance is the success of boosting \citep{Freund:1997,
  Friedman:2001}, bagging \citep{Breiman:1996}, random forests
\citep{Breiman:2001}, and related techniques
\citep[e.g.,][]{Chipman:2010} to aggregate so-called ``weak
learners''.  These approaches to classification and prediction have
been advertised as the ``best off-the-shelf classifier[s] in the
world'' \citep{Breiman:1996}, and are equally powerful in prediction
tasks.

While the advantages of collating information from multiple sources
are manifold, it is nevertheless false to assume that more is always
better \citep[c.f.,][]{Page:2011}.  Not all guesses are equally
informative, and naive approaches to collating forecasts risks both
overvaluing wild guesses and undervaluing unusual that are nonetheless
sometimes correct.  

The particular ensemble method we are extending is ensemble Bayesian
model averaging (EBMA). First proposed by \citet{Raftery:2005}, EBMA
pools forecasts as weighted combination of predictive probability
distribution functions (PDFs).  Rather that selection some ``best
model,'' EBMA collects \textit{all} of the insights from multiple
forecasting efforts in a coherent manner via statistical post
processing.  The weight assigned to each component forecast, reflects
two aspects its past predictive accuracy and uniqueness. 

In recent years, variants of ensemble Bayesian model averaging or other methods of combining predictions have been at the forefront of research on forecasting. In recent years, variants of the EBMA method have been applied to subjects as diverse as inflation \citep{Wright:2009, Koop:2010, Gneiting:2010},
stock prices \citep{Billio:2011}, economic growth and policymaking
\citep{Brock:2007, Billio:2010}, exchange rates \citep{Wright:2008},
industrial production \citep{Feldkircher:2010}, ice formation
\citep{Berrocal:2010}, visibility \citep{Chmielecki:2010}, water
catchment streamflow \citep{Viney:2009}, climatology \citep{Min:2006,
  Min:2007, Smith:2009}, and hydrology \citep{Zhang:2009}. 

  Indeed, while EBMA methods have been applied to predictions on a number of different topics, research is underway to extend the method in a variety of ways. EBMA has been adjusted to handle missing data
\citep{Fraley:2010, Mccandless:2011}, to incorporate spatial information \citep{Feldman:2012} as well as calibrate model
weights on non-likelihood criteria \citep[e.g.,][]{Vrugt:2006}.
\citet{MoellerEtAl:2012} introduce statistical postprocessing of EBMA models for univariate forecasts into jont predictive distributions using Gaussian copulas. This can be especially valuable when prediction different processes that are correlated with each other. \citet{RingsEtAl:2012} have tweaked EBMA in a different direction, allowing for time-specific variances and utilizing particle filtering methods. All in all, while EBMA allows for improvements in forecasts over single models, various avenues for further improvements exist.  

Although there are clear advantages to combining forecasts via EBMA,
several challenges common to the social sciences pose a particular
challenge to more widespread adoption of the method.  To begin with,
the amount and quality of data for calibrating ensembles is far from
ideal.  EBMA was first developed for use in weather forecasting where
measurement of outcomes is fairly precise and data abundant.
Predicting, for instance, water surface temperatures in 200 locations
across just five days provides 1,000 observations by which model
weights can be calibrated.  In contrast, forecasting quarterly GDP
growth in the United States for five \textit{years} provides only 20.

A second, and related, issue is dimensionality. Many prediction tasks
involve many forecasts predicting few, or even just one, outcome.  For
example, in the field of economics, a wide variety of consulting
firms, banks, and international organizations provide multiple
forecast for various economic quantities such as the unemployment, GDP
growth, and inflation.  Indeed, the Federal Open Market Committee
(FOMC) of the U.S. Federal Reserve Board itself generates over a dozen
forecasts of multiple key economic indicators for the next three
years.\footnote{For a recent sample of these forecasts, see:
  \url{http://1.usa.gov/zjyisV}.}

A final issue is the inconsistency with which forecasts are
issued. Given the lengthy time periods often involved, there are
likely to be many missing forecasts in any given time window.
Moreover, we cannot assume that forecasts for any time period from a
specific model or team are missing at random.  Particularly
unsuccessful forecasts may be suppressed and some forecasting efforts
are only active for short time-periods due to poor performance.
Moreover, forecasts have tended to accumulate with more potential
components being available for more proximate time periods.

While particularly egregious for specific application (c.f.,
presidential election forecasting), these data issues are endemic to
the social sciences.  Moreover, they are not benign.  As we
demonstrate below, calibrating large ensemble models on sparse (and
even incomplete) data leads to miss-specification of EBMA model
weights and decreased out-of-sample predictive performance.  We
therefore ... TRANSITION NEEDED.

\section{EBMA for sparse data} 
\label{model}


As its name suggests, EBMA descends from Bayesian model averaging
(BMA) methodology \citep[c.f.,][]{Madigan:1994, Raftery:1995,
  Hoeting:1999, Clyde:2003, Clyde:2004}, which was first introduced to
political science by \citet{Bartels:1997} and has been applied in a
number of contexts \citep[e.g.,][]{Bartels:2001, Gill:2004,
  Imai:2004}. \ifthenelse{\boolean{blind}}{\citet{Author:2020b}}{\citet{Montgomery:2010c}}
provide a more in-depth discussion of BMA and its applications in
political science.  A more detailed discussion of the basic EBMA model
extended here is provided in \citet{mhw:2012}.

\subsection{Baseline EBMA model}

Assume we have some quantity of interest to forecast,
$\mathbf{y}^{t^*}$, in future period $t^\ast \in T^\ast$.  Further
assume that we have extant forecasts for events $\mathbf{y}^t$ for
some past period $t \in T$ that were generated from $K$ forecasting
models or teams, $M_1, M_2, \ldots, M_K$, for which have a prior
probability distribution $M_k\sim \pi(M_k)$. The PDF for
$\mathbf{y}^t$ is denoted $p(\mathbf{y}^t|M_k)$.  Under this model,
the predictive PDF for the quantity of interest is
$p(\mathbf{y}^{t^*}|M_k$), the conditional probability for each model
is $p(M_k|\mathbf{y}^t) =
p(\mathbf{y}^t|M_k)\pi(M_k)/\underset{k=1}{\overset{K}{\sum}}p(\mathbf{y}^t|M_k)\pi(M_k)$,
and the marginal predictive PDF is $p(\mathbf{y}^{t^*}) =
\underset{k=1}{\overset{K}{\sum}}
p(\mathbf{y}^{t^*}|M_k)p(M_k|\mathbf{y}^{t})$.  This can be viewed as
the weighted average of the component PDFs where the weights are
determined by each model's performance within the already-observed
period $T$.

The EBMA procedure assumes $K$ forecasting models throughout the
training ($T^{\prime}$) calibration ($T$) and test ($T^\ast$)
periods. The component models are created based on data from the
training period $T^\prime$. The component model predictions for the
calibration period T are then generated out-of-sample. The goal is to
estimate the parameters for the ensemble prediction model using
$\mathbf{f}^{t}_k$ for period $T$.  It is then possible to generate
true ensemble forecasts ($\mathbf{f}_k^{t^\ast}$) for observations in
the test period $t^\ast \in T^*$.  This approach allows us to weight
models based on their out-of-sample predictive power, thus implicitly
penalizing overly-complex ``garbage can'' models.  One of the distinct
advantages of EBMA is that it does not require researchers to develop
metrics to penalize component forecasts for complexity or even to have
access to the details of the component forecasting methods themselves.

Let $g_k(\mathbf{y}|\mathbf{f}_k^{s|t, t^\ast})$ represent the
predictive PDF of component $k$, which may be the original prediction
from the forecast model or the bias-corrected forecast.  The EBMA PDF
is a finite mixture of the $K$ component PDFs, denoted
$p(\mathbf{y}|\mathbf{f}_1^{s|t}, \ldots,
\mathbf{f}_K^{s|t})=\overset{K}{\underset{k=1}{\sum}} w_k
g_k(\mathbf{y}|\mathbf{f}_k^{s|t})$, where $w_k \in [0,1]$ are model
probabilities, $p(M_k|\mathbf{y}^t)$, and $\sum_{k=1}^Kw_k=1$. The
ensemble predictive PDF with this notation is then
$p(y|f_{1}^{t^\ast}, \ldots,
f_{K}^{t^\ast})=\overset{K}{\underset{k=1}{\sum}} w_k
g_k(y|f_{k}^{t^*})$.\footnote{Past applications have statistically
  post-processed the predictions for out-of-sample bias reduction and
  treated these adjusted predictions as a component
  model. \citet{Raftery:2005} propose approximating the conditional
  PDF as a normal distribution centered at a linear transformation of
  the individual forecast, $g_k(\mathbf{y}|\mathbf{f}_k^{s|t}) =
  N(a_{k0} + a_{k1}\mathbf{f}_k^{t}, \sigma^2)$. However, in the
  presence of sparse data, including the additional $\mathbf{a}$
  parameters risks over-fitting and reduced predictive performance.
  We therefore use a simpler formulation.}
For the applications below, we assume
$g_k(\mathbf{y}|\mathbf{f}_k^{s|t}) = N(\mathbf{f}_k^{t}, \sigma^2)$,
where $\sigma$ is a common variance component across components.
Thus, the ultimate predictive distribution for some observation
$y^{t^\ast}$ is

\begin{equation}
\label{pdf}p(y|f_1^{s|t^\ast},
\ldots, f_K^{s|t^\ast}) = \overset{K}{\underset{k=1}{\sum}} w_k
N(f_k^{t^\ast}, \sigma^2).
\end{equation}

\noindent This, is a weighted mixture of $K$ normal distributions each with 
means  determined by $\mathbf{f}^{t^\ast}$ and scaled by the
model weights $\mathbf{w}$.

\subsection{Model estimation}

FLORIAN: THIS SECTION NEEDS TO BE CHANGED A BIT SO IT IS NOT A DIRECT
COPY OF THE FIRST ARTICLE.  DONT CHANGE THE MATH, JUST THE WORDS.


Since the component model forecasts, $f^t_1, \ldots, f^t_k$, are
pre-determined, the EBMA model is fully specified by estimating model
weights, $w_1, \ldots, w_k$ and the common variance parameter
$\sigma^2$.  We estimate these using maximum likelihood methods
\citep{Raftery:2005}.  The log likelihood function,

\begin{equation}
\mathcal{L}(w_1, \ldots, w_k, \sigma^2)=\sum_t\log\left(\sum_{k=1}^Kw_kN(f^t_k, \sigma^2) \right),
\end{equation}


\noindent cannot be maximized analytically.  \citet{Raftery:2005}
propose an EM algorithm which expresses EBMA as a finite mixture model
\citep{mclachlan:peel:2000,imai:tingley:2012}.  We introduce the
unobserved quantities $z_k^t$, which represents the probability that
observation $y^t$ is ``best'' predicted by model $k$.  The E step
involves calculating estimates for these unobserved quantities using
the formula
\begin{equation}
\label{E-step}
\hat{z}^{(j+1)t}_{k} = \frac{\hat{w}^{(j)}_k
p^{(j)}(y|f_{k}^{t})}{\overset{K}{\underset{k=1}{\sum}}\hat{w}^{(j)}_kp^{(j)}(y|f_{k}^{t})},
\end{equation}
\noindent where the superscript $j$ refers to the $j$th iteration of
the EM algorithm.

Note that $w_k^{(j)}$ is the estimate of $w_k$ in the $j$th iteration and
$p^{(j)}(.)$ is shown in \eqref{pdf}.  Assuming these estimates of
$z_{k}^{s|t}$ are correct, it is then straightforward to derive the
maximizing value for the model weights. Thus, the M step estimates
these as 

\begin{equation}
\label{M-step}
\hat{w}^{(j+1)}_k=\frac{1}{n}\underset{t}{\sum}\hat{z}^{(j+1)t}_{k},
\end{equation}

\noindent where $n$ represents the number of observations in the
calibration dataset.  Finally,

\begin{equation}
\label{sigma}
\hat{\sigma}^{2(j+1)}=\frac{1}{n}\underset{t}{\sum}\overset{K}{\underset{k=1}{\sum}}\hat{z}^{(j+1)t}_{k}(y-f_{k}^{t})^2.
\end{equation}
\noindent The E and M steps are iterated until the improvement in the
log-likelihood is no larger than some pre-defined tolerance.  We
initiate the algorithm with the assumption that all models are equally
likely, $w_k = \frac{1}{K} ~ \forall ~ k \in [1, \ldots, K]$ and
$\sigma^2=1$.

\subsection{Adjustments for sparse data}

When ensembles are calibrated on very few observations, there is an
increased chance that EBMA may miss-weight component models in a way
that reduces out-of-sample performance due to unusually poor or strong
predictive performance in the limited calibration sample. This is
especially true when the short calibration period is combined with
missing observations in component model
predictions.\footnote{Adjustments to the baseline model to accomodate
  missing components is provided in Appendix A.q}

To improve the performance of EBMA in the context of sparse data, we
propose a ``wisdom of crowds'' parameter, $c \in [0,1]$, that reflects
our prior belief that all models should receive some, but not
necessarily equal, weight. We rescale $z^t_k$ to have a minimum value
$\frac{c}{K}$.  This states that there is, at a minimum, a
$\frac{c}{K}$ probability that observation $t$ is correctly
represented by each model $k$.  Since
$\overset{K}{\underset{k=1}{\sum}} z_k^t = 1$, this implies that
$z_k^t \in [\frac{c}{K}, (1-c)]$.  To achieve this, we replace
Equation \ref{M-step} above with

\begin{equation}
\hat{z}^{(j+1)t}_{k} = \frac{c}{K} + (1-c)\frac{\hat{w}^{(j)}_k
p^{(j)}(y|f_{k}^{t})}{\overset{K}{\underset{k=1}{\sum}}\hat{w}^{(j)}_kp^{(j)}(y|f_{k}^{t})}.
\end{equation}

Note that when $c=1$, all models are considered equally informative
about the outcome and $w_k=\frac{1}{K} \forall K$. Thus, we see that
the arithmetic mean or median of component forecasts for time period
$t$ represents a special case of EBMA where $c=1$.\footnote{The mean
  or median would be equivalent depending on if the posterior mean or
  median is used to make a point prediction.}  Likewise, the general
EBMA discussed in \citet{mhw:2012} represents a special case of this
more general model where $c=0$.

\section{Simulations and applications}
\label{empirics}

The introduction of the ``wisdom of crowds'' parameter to the base
EBMA model is designed to improve out-of-sample predictive performance
in the context of data-quality challenges common to social science
applications.  In particular, it is designed to address poor weight
calibrations that are common when the size of the calibration sample
is small, there are missing components in the calibration sample, and
the number of component forecasts for which weights must be estimated
is large.  Above, we claim that this increases the miss-estimation of
component model weights and decrease the predictive performance of
EBMA.

To justify these claims, in this section we present the results of a
simulation study of our modified EBMA algorithm and two empirical
applications of the modified method.  We begin with a simulation that
illustrates the reduced predictive performance of the baseline EBMA
model in the circumstances described above and illustrates the
improvements that result from our proposed modification.  We then
apply our method to, first, the prediction of the US unemployment
rate, and, second, to the prediction of the 2012 US presidential
election.

\subsection{Simulation study} 

In this section, we conduct a simulated study of the adjusted EBMA
model proposed above.  These simulations serve two purposes.  First,
they demonstrate the challenges presented to ensemble forecasting when
calibration samples are small and the number of forecasting models are
large.\footnote{To reduce the parameter space for these simulations,
  we limit ourselves here to exploring the roll of calibration sample
  sizes and number of component forecasts.  We do not consider issues
  of missingness.}  Second, it explores the extent to which our
modified EBMA algorithm ameliorates these difficulties.  In addition,
we provide some guidance regarding the selection of $c$.

The simulations are designed to reflect the ``best possible'' world
for the baseline EBMA model.  The distribution of outcomes is drawn
precisely from the mixture distribution shown in Equation~(\ref{pdf})
where $\sigma^2 = 1$ and the individual component forecasts are drawn
from the multivariate normal distribution $N(\mathbf{0}_K,
\mathbf{I}_K)$.  Moreover, we assume that the true data generating
process, both in-sample and out-of sample, involves \textit{only} the
$K$ forecasting models which are themselves estimated with perfect
precision.  The ``true'' model weight for each simulation are drawn
from a Dirichlet distribution with $K$ categories and concentration
parameter $\mathbf{\alpha} = (10, 5, 3, \frac{1}{K-3})$ when $K>3$ and
$\mathbf{\alpha} = (10, 5, 3)$ when $K=3$.  This ensures that the
model weights always sum to 1, but that there is still some
heterogeneity in the true model weights.  We varied the size of the
calibration sample ($n_T$), the number of component forecasts ($K$),
and the wisdom of crowds paramter ($c$).  This last is used only for
model estimation, and plays no role in the creation of the simulated
data itself.

\begin{table}
  \caption{Parameters for simulation}
\label{params}
\small
\centering

\vspace{.2cm}
\begin{tabular}{lll}
  \toprule
  Parameter & Meaning & Values \\
  \midrule
  $n_{T}$ & Sample size in calibration period $T$  & 3-15,20,25,35,45,55,65,85,100 \\
  $n_{T^\ast}$ &  Sample size in test period $T^\ast$&$250$\\
  $K$ & \# of component forecasts & 3,5,7,9,11,13,15\\
  $\sigma^2$ & Common variance component & 1 \\
  $\alpha$ & Weight concentration parameter  &$ (10, 5, 3, \frac{1}{K-3})$ \\
 $c$ & Wisdom of crowds parameter & 0,0.01,0.02,0.03,0.04,0.05,0.075,0.1 0.15,0.2,0.3,0.5\\
  $M$ & Simulations at each setting & 100 \\
  \bottomrule
\end{tabular}
\end{table}

For each simulation, we generate component forecasts for both the
calibration and test period.  We fit an EBMA model as specified above
to the calibration sample data only.  We then generate out-of-sample
predictions for the 250 observations in the test period using the
fitted model and compare then them to the true values from the
simulated data.  

We begin by examining the accuracy of the baseline EBMA ($c=0$)
predictive PDF shown in Equation~\ref{pdf} for different values of $K$
(the number of components) and $n_{T}$ (the calibration sample size) .
We focus here on the CRPS measure because it is awesome.  FLORIAN:
BRIEF DESCRIPTION OF CRPSIN MAIN TEXT HERE. MATHEMATICAL DETAILS IN
THE APPENDIX.  CITE RELEVANT WORK.  EXPLAIN INTUITION OF WHY IT IS
BEST. "Another candidate statistic that can be used to derive model weights is the continuous ranked probability score (CRPS). In an ideal world perfect forecasters would correctly anticipate all future events,
and probability mass of one would be centred on the soon-to-be realised outcomes. The corresponding cumulative density functions would be step functions, with the steps also located at the realised
outcomes. The CRPS can be conceptualised as a measure of the deviation from this step-function ideal. Two forecast densities may have the same score – the same slope of the CDF evaluated at the
realisation – but one density may be considered better than another because it has more probability
mass ‘near’ the realisation." this is from 

\begin{figure}[ht]
\caption{A plot to show that error is a function of $K$ and $N_T$}
\label{simplot1}
\centering
\includegraphics[scale=.8]{SimTemp1}
\end{figure}

Figure~\ref{simplot1} shows how the out-of-sample performance of the
EBMA method, as measured by CRPS, depends significantly on the number
of forecast models included as well as the calibration sample size.
First, note the upward slope of each line, indicating that the
predictive power of the model decreases as the number of components in
the true data generating process increases.  That is, as the number of
model parameters that \textit{must} be correctly estimated to make
accurate predictions increases, the quality of the forecast goes down.
Second, Figure~\ref{simplot1} shows out-of-sample CRPS when $n_T=5$
(black), $n_T=11$ (blue), and $n_T=100$ (red).  For all values of $k$,
the CRPS is a decreasing function of $N_T$.  This illustrates that the
performance of the baseline EBMA model improves as the calibration
sample grows.

\begin{figure}[ht]
\caption{A plot to show that $c$ helps in some situations}
\label{simplot2}
\centering
\includegraphics[scale=.8]{SimTemp2}
\end{figure}

The remaining question, then, is to what degree adding the ``wisdom of
crowds'' parameter to the baseline model improves performance.  To
answer, we examined the out-of-sample predictive performance of EBMA
for differing values of $c$. Figure~\ref{simplot2} shows the median
CRPS recorded for differing values of $N_T$, $K$, and $c$.  For the
purposes of clarity, we focus on only three subsets from the
simulations: $n_T=3, K=15$, $n_T=5, K=9$, $n_T=35, K=5$.  While this
is far from a complete analysis of the simulated data, it does serve
the limited purposes of demonstrating that, in some circumstances, the
``wisdom of crowds'' parameter aids prediction under some
circumstances.

There are two aspects of Figure~\ref{SimTemp2} that are particularly
salient.  First, note that the addition of $c$ to the base EBMA
modeldoes not uniformly aid in out-of-sample performance.  When the
calibration sample is modestly large ($N_T=35$) and there are few
models to calibrate, the addition of $c$ uniformly decreases model
performance.  However, with small calibration samples and modestly
large numbers of component models, the addition of $c$ appears to aid
predictive performance.  Second, the relationship between $c$ and CRPS
is non-monotonic.  CRPS decreases for small to modest values of $c$,
but eventually begins to rise.  Based on our examination of the
broader set of simulations, we therefore recommend the selection of
values of $c \in [0, 0.1]$.  The simulations favor smaller values of
$c$ as the ratio of $N_T$ to $K$ increases.  As a default choice, we
recommend $c=0.05$.  Researchers may also choose the value of $c$
based on a k-fold cross-validation study of the calibration sample.

 Bearing these results in mind, we now turn to examining how these
methods work in two areas that exemplify forecasting in the social
sciences. The first is the prediction of unemployment in the United
States, and the second in the area of predicting the vote for the
incumbent party in U.S. presidential elections. Both areas have well
developed forecasting traditions in the scholarly and policy
community.


\subsection{Quarterly unemployment in the United States}
\label{econ}

Forecasting macroeconomic variables is a quite common exercise in the
field of economics and statistics. Calculating accurate forecasts of
economic variables is a necessity for policy makers and
businesses. These forecasts are created using a wide variety of
statistical models.\footnote{For a more comprehensive overview on
  foresting of economic variables and time-series forecasting see
  \citet{Elliott:Timmermann:2008} and \citet{Goijer:Hyndman:2006}.}
The majority of scholars employ time-series models, with the most
commonly applied statistical method being autoregressive integrated
moving average (ARIMA) and vector autoregressive (VAR) models. The
sophistication and complexity of forecasting models has increased
considerably over time. In particular, non-linear dynamic models have
gained prominence including threshold autoregressive models, Markov
switching autoregressive models and smooth transition autoregression
\citep{Elliott:Timmermann:2008,Montgomery:etal:1998}. More recently,
forecasters have introduced Bayesian VAR models and state-space models
to their arsenal \citep{Goijer:Hyndman:2006,Elliott:Timmermann:2008}.

Unsurprisingly, given the large number of ongoing forecasts, scholars
have attempted to improve predictive accuracy by combining forecasts
\citep{Bates:1969, Palm:Zellner:1992, Elliott:Timmermann:2008}.
Recently, EBMA and related Bayesian model averaging methods have been
successfully employed to create ensemble forecasts of various
macroeconomic indicators including inflation
\citep{Koop:2010,Wright:2009}, GDP \citep{Billio:2010}, stock prices
\citep{Billio:2011}, and exchange rates \citep{Wright:2008}.

Policy makers too have come to rely on ensemble forecasts of a sort.
The desire to aggregate the collective wisdom of multiple forecasting
teams is apparent in the \textit{Survey of Professional Forecasters
  (SPF)} published by the \textit{Federal Reserve Bank of
  Philadelphia}.  The \textit{SPF} includes forecasts for a large
number of macroeconomic variables in the U.S., including the
unemployment rate, inflation, and GDP growth.\footnote{The
  \textit{SPF} was first administered in 1968 by the American
  Statistical Association and the National Bureau of Economic Research
  (NBER).  Since 1990, however, it is run by the Federal Reserve Bank
  of Philadelphia.
  \url{http://www.phil.frb.org/research-and-data/real-time-center/survey-of-professional-forecasters/}}
In the first month of every quarter, a survey is sent to selected
forecasters and is returned by the middle of the second month of the
quarter. Forecasts are made for the current quarter as well as several
quarters into the future.

This plethora of predictions seems ideal for applying EBMA.
Nonetheless, it is plagued by the same issues as discussed in
Section~\ref{theproblem}.  Even with quarterly measures, there are
relatively few observations, many forecasting teams, and a significant
number of missing observations.  This setting, therefore, provides a
test bed for the adjusted EBMA model discussed above.

Here we focus on forecasting the civilian unemployment rate (UNEMP) as
published by the \textit{SPF}. For this application, we selected the
forecast horizon to be four quarters into the future, i.e. predictions
made in the first quarter of 2002 are for the first quarter of 2003
and so on. In total, the \textit{SPF} data on unemployment contains
forecasts by 569 different teams. However, for any quarter, the
average number of forecast teams making a prediction for four quarters
into the future is quite small and the majority of observations for
any given quarter are missing.\footnote{On average only 8.4 per cent
  of all teams make a forecast for any one quarter.}

To provide a meaningful benchmark, we also include the ``Green Book''
forecasts produced by the Federal Reserve. These forecasts are made by
the research staff of the Board of Governors and are handed out prior
to meetings of the Federal Reserve Open Market Committee
(FOMC).\footnote{One issue with forecast evaluation in many domains in
  economics is that the macroeconomic data (i.e. our ``true
  observations'') are revised regularly. The unemployment rate for a
  given quarter at that time is generally an estimate that is subject
  to revision when better data becomes available. When evaluating
  forecasts, it is thus important whether predictions are compared to
  the outcome data for each quarter available at the time or whether
  the revised and most recent data is used. As
  \citet{Croushore:Stark:2001} describe, depending on the forecast
  exercise, it can make a difference whether the forecast models are
  evaluated using ``real-time'' (original estimate) or the ``latest
  available'' (revised) data. We have decided here to use the ``latest
  available'' data and do not believe that it should make a difference
  in our case, as all predictions are evaluated against the same data
  and EBMA is a mixture of the component forecast models. Thus the
  component models and our benchmark model are estimated and evaluated
  on the same data. However, in a future version of this paper we will
  replicate this analysis using real-time data to evaluate the
  forecasts.} % Better now?

Taking the \textit{SPF} and Green Book unemployment forecasts, we
calibrate an ensemble model for each period $t$, using forecaster
performance over the past ten quarters.  Only forecasts that had made
predictions for five of these quarters were included in the ensemble.
Thus, the EBMA model uses only 163 models out of a possible 293
forecasting models that made predictions during the period we study.
Due to missing data early in the time series, and the fact that Green
Book forecasts are sequestered for five years, we generate forecasts
beginning in the third quarter of 1983 and running through the fourth
quarter of 2007.



\begin{figure}[h]
\caption{Observed and forecasted U.S. unemployment (1981-2007)}
\label{timeSeries}
\begin{center}
\includegraphics[scale=.8]{mdwtimeSeries2}
\end{center}
\end{figure}


One approach to evaluating the performance of EBMA is to compare its
predictive accuracy to that made by other systematic forecasting
efforts and methods of generating ensemble predictions.  Specifically,
we compare EBMA's ($c=0.05$) predictive accuracy to (1) the Green
Book, (2) the median forecaster prediction and (3) the mean forecaster
prediction.\footnote{Note that the EBMA model is calculated on only a
  the subset of forecasts that have made a sufficiently large number
  of recent predictions to calibrate model weights.  Thus, the median
  forecast and the ensemble forecast will not be the same even when
  $c=1$.  } 

Figure~\ref{timeSeries} shows a visual representation of the
Greenbook, median SPF and the EBMA (with $c=0.05$) forecasts over
time, as well as the true unemployment rate. As was noted above and is
clearly visible, the SPF and Greenbook forecasts are quite
similar. \citet{Baghestani:2008} noted that the Greenbook forecast is
slightly biased to over predict the unemployment rate. In some periods
EBMA is able to correct this bias, however given the similarity of
component models, the improvement in that direction is rather
small. In general, however, it is easily visible that the EBMA
forecast is closer to the actual rate than the median SPF or the Green
Book forecast.

\begin{table}[h]
\caption{Comparing adjusted EBMA models with Green Book, median, and mean forecasts of U.S. Unemployment (1981-2007)}
\begin{center}
\begin{tabular}{lrrrrrrrr}
\toprule
 & MAE & RMSE & MAD & RMSLE & MAPE & MEAPE & MRAE & PW \\ 
\midrule
 EBMA (c=0)& 0.54 & 0.74 & 0.37 & 0.093 & 8.37 & 6.49 & \textbf{0.73} & \textbf{27.36} \\ 
  EBMA (c=0.05)& \textbf{0.54} & 0.74 &\textbf{ 0.37} & \textbf{0.093} & \textbf{8.33} & \textbf{6.30} & 0.75 & \textbf{27.36} \\ 
 EBMA (c=0.1)& 0.54 & 0.74 & 0.35 & 0.093 & 8.40 & 6.44 & 0.76 & 28.30 \\ 
EBMA (c=1) & 0.61 & 0.80 & 0.46 & 0.102 & 9.72 & 8.92 & 0.95 & 46.23 \\ 
 Green Book& 0.57 & \textbf{0.73} & 0.43 & 0.093 & 9.37 & 8.81 & 1.00 & 45.28 \\ 
 Forecast Median& 0.62 & 0.81 & 0.47 & 0.103 & 9.83 & 8.87 & 0.98 & 47.17 \\ 
Forecast Mean& 0.61 & 0.80 & 0.46 & 0.102 & 9.71 & 9.06 & 0.93 & 46.23 \\ 
\bottomrule
\end{tabular}
\end{center}

\label{compareTable1}
Definitions of model fit statistics are provided in the Appendix. The model with the lowest score for each metric are shown in bold.  Differences between model performance may not be obvious due to rounding.
\end{table}


Table~\ref{compareTable1} formally compares these baseline models
using all eight of the metrics to EBMA models with $c=$0, 0.05, 0.1,
and 1 respectively.  To do this, we focus on eight model fit indices
available in the literature.  The eight metrics we use are mean
absolute error (MAE), root mean squared error (RMSE), median absolute
deviation (MAD), root mean squared logarithmic error (RMSLE), mean
absolute percentage error (MAPE), median absolute percentage error
(MEAPE), median relative absolute error (MRAE) and percent worse (PW).
The latter two metrics are measured relative to a naive model, simply
predicting the future rate of unemployment as being the same as the
current rate of unemployment.  Further details for these metrics are
shown in the Appendix \citep{brandt:freeman:schrodt:2011}.


The bolded cells in each column of Table~\ref{compareTable1} indicate
the model that performed ``best'' as measured by each metric.  With
one exception, (the Green Book outperforms the ensemble by 0.01 on
RMSE), the EBMA model outperforms both the Green Book forecast and the
unweighted mean and median forecast on every metric.  Moreover, these
results confirms that the $c$ parameter is best set to a small number.
In general, the model with $c=0.05$ performs best (or is tied for
best) on six out of eight of these metrics.


% \begin{figure}[!htb]
%   \caption{Ensemble weights for SPF forecasts of U.S. unemployment
%     with a rolling calibration window}
% \label{modelWeights}
% \begin{center}
% \includegraphics[scale=.70]{awesome}
% \end{center}

% \footnotesize This graphic illustrates the component weights for each EBMA
% model estimated between 1983 and 2007. Ensembles are calibrated on the
% past ten quarters. The colors range from blue to red to indicate
% increasing weights for components in a given ensemble model. Those
% components excluded in a given model are missing.

% \end{figure}

% Figure~\ref{modelWeights} provides a visual representation of EBMA
% model calibrations throughout this period.  In this figure, the wisdom
% of crowds tuning parameter is set to a modest $c=0.05$.  The colors
% indicate the model weight assigned to each component on a red-blue
% color ramp (components not included in the ensemble are blank).
% Models assigned no weight are shown in dark blue while models that are
% heavily weighted are shown in red. Figure~\ref{modelWeights} also
% illustrates the difficulties inherent in forecasting with this type of
% data.  For any given year, only a subset of forecasting teams offer a
% prediction.  Further, an even smaller subset contains models that both
% offer a prediction and have made a sufficiently large number of prior
% forecasts to facilitate model calibration.  Finally, the very
% sparseness of the data encourages the ensemble model to place a very
% large amount of weight on the best performing models.



\begin{figure}[!htb]
\caption{Comparing predictive accuracy of EBMA and component models with eight metrics}
\label{compare2Components}
\begin{center}
\includegraphics{compare2Components}
\end{center}

\footnotesize
The top panel plots EBMA's relative performance, as measured by eight
forecasting metrics, to each of its components against the number of
forecasts generated by the component models.  The bottom panel shows
the percentage of component models that EBMA matches or outperforms as
measured by each metric.  Details on the eight forecasting metrics are
shown in the Appendix.  The Green Book forecast is labeled (GB) and is
located in the upper left portion of the top panel).


\end{figure}

We now turn to evaluating the performance of the ensemble relative to
its 163 component forecasts.  It is important to note that many of
these forecasters make predictions in a relatively small subset of
cases.  That is, each model $k$ offers forecasts for only a subset of
cases $n_k \subset n$.  To create a fair comparison, therefore, we
calculate these fit indices only for $n_k \forall k \in [1,K]$.  By
this measure, the EBMA model performs very well.
Figure~\ref{compare2Components} provides a summary of these results.
The top panel shows the percentage of metrics by which EBMA
outperforms each component. The bottom panel shows the percentage of
component models that EBMA ``beats'' as measured by each metric.

Notably, the relative superiority of EBMA to its components is
somewhat less for components that provide few forecasts.  This
reflects the fact that with so many forecasts, some are likely to be
more accurate than the ensemble by chance alone. Additionally, when
the number of forecasts is low it is likely that a given model
received less weight than it ``deserves'' given the model's
performance.\footnote{See the Appendix for a discussion of how EBMA
  handles missing component forecasts.}  However, across a large
  number of forecasts, EBMA significantly outperforms any of its
  components, including the Green Book (GB).  It is also worth noting
  that only 6 out of the total 163 components outperforms EBMA on
  every metric.




\subsection{U.S. presidential elections}

We now turn to the task of combining expert predictions of
U.S. presidential election.\footnote{See also \citep{Montgomery:2012c}.}
This example provides a clear illustration of the difficulties of
creating ensemble forecasts in the social sciences and allows us to
further illustrate the advantages of generating predictive PDFs when
focusing on a limited number of important events.

Predicting U.S. presidential elections is, perhaps, the quintissential
forecasting task that combines all of the issues discussed in
Section~\ref{theproblem}.  Table~\ref{tab:one} represents nearly the
entirety of scholarly forecasts which produced more than one
out-of-sample forecast for elections in the 20th century prior to the
2012 election.\footnote{ See, for example \citet[][]{Fair:2009,
    Fair2011, Abramowitz:2008, Campbell:2008,
    Cuzan:2004,Cuzan:Bundrick:2008,hibbs:2012, Lockerbie:2008,
    Erikson:Wlezien:2008, Graefe:2010, Holbrook:2008}.  A recent
  symposium in {\em PS: Political Science \& Politics} presents and
  summarizes attempts by a variety of scholars to predict the 2012
  U.S. presidential election. In a symposium contribution, we use the
  in-sample fitted values of the election forecasting models to
  calibrate the EBMA model \citep{Montgomery:2012c}. However, the
  strength of EBMA is greatest when the model is calibrated on true
  out-of-sample forecasts as we do here.}  In this instance, we have
only five observations by which to calibrate an ensemble model, while
we have nine forecasting models.  Moreover, several of the individual
forecasts are missing for a significant portion of the data.  The
forecast of Cuz\`an, for instance, is missing for 60\% of the
elections in this dataset.\footnote{The predictions by Cuz\`an for
  2004 stems from the FISCAL model published prior to the 2004
  election by \citet{Cuzan:2004}, while the 2008 prediction comes from
  the FPRIME short model presented in advance of the election
  \citep{Cuzan:Bundrick:2008}. However, both models are quite similar
  in their composition.}

\begin{table}[ht]
\caption{Pre-election forecasts of the percent of the two-party vote going to the incumbent party in U.S. Presidential elections}
\label{tab:one}
\footnotesize
\begin{center}
\begin{tabular}{rlrrrrrrrrr}
  \toprule
  & F & A & C & H & LBRT & L & Hol & EW & Cuz \\ 
  \midrule
  1992 & 55.7 & 46.3 & 49.7 & 48.9 & 47.3 &  &  &  &  \\ 
  1996 & 49.5 & 57.0 & 55.5 & 53.5 & 53.3 &  & 57.2 & 55.6 &  \\ 
  2000 & 50.8 & 53.2 & 52.8 & 54.8 & 55.4 & 60.3 & 60.3 & 55.2 &  \\ 
  2004 & 57.5 & 53.7 & 52.8 & 53.2 & 49.9 & 57.6 & 55.8 & 52.9 & 51.1 \\ 
  2008 & 48.1 & 45.7 & 52.7 & 48.5 & 43.4 & 41.8 & 44.3 & 47.8 & 48.1 \\ 
  \bottomrule

\end{tabular}
\end{center}
Forecasts were published prior to each election by \textbf{F}air, \textbf{A}bramowitz, \textbf{C}ampbell, \textbf{H}ibbs, \textbf{L}ewis-\textbf{B}eck and \textbf{R}ice (1992), Lewis-Beck and \textbf{T}ien  (1996-2008),   \textbf{L}ockerbie, \textbf{Hol}brook, \textbf{E}rikson and \textbf{W}lezien and \textbf{Cuz}\`an and Bundrick.  Data were taken from the collation presented at \url{http://fivethirtyeight.blogs.nytimes.com/2012/03/26/models-based-on-fundamentals-have-failed-at-predicting-presidential-elections/}. FLORIAN: WE NEED TO VERIFY ALL OF THESE NUMBERS AND ADD CITATIONS TO THE RELEVANT WORK ... FOOTNOTE?
\end{table}


Using the forecasts shown in Table 1, we fit an EBMA model with
$c=0.05$.  The model weights and in-sample fit statistics for the
ensemble and its components are shown in Table~\ref{presModel}.  As
can be seen, the EBMA model assigns the majority of weight to the
Abramowitz model with the model by Campbell receiving the second
largest weight. These weights are based on the performance of each
model in forecasting the incumbent vote share in the presidential
elections between 1992 and 2008. The Cuz\`an and Bundrick model is
weighted to such a small degree because only out-of-sample predictions
for 2004 and 2008 were available here.


% latex table generated in R 2.15.1 by xtable 1.7-0 package
% Sun Aug 12 21:58:55 2012
\begin{table}[ht]
\caption{Model weights and in-sample fit statistics for EBMA model of U.S. Presidential Elections (1992-2008)}
\label{presModel}
\begin{center}
\begin{tabular}{lrrrr}
  \toprule
 & \shortstack{EBMA\\ Weight}&RMSE &MAE \\ 
  \midrule
EBMA &  & 1.92 & 1.56 \\ 
  Fair & 0.02 & 5.53 & 4.58 \\ 
  Abramowitz & 0.78 & 2.02 & 1.72 \\ 
  Campbell  & 0.07 & 3.46 & 2.88 \\ 
  Hibbs  & 0.04 & 2.68 & 2.44 \\ 
  Lewis-Beck, Rice, and Tien & 0.06 & 2.78 & 2.28 \\ 
  Lockerbie  & 0.00 & 7.33 & 6.97 \\ 
 Holbrook & 0.01 & 5.73 & 4.77 \\ 
  Erikson and Wlezien & 0.02 & 2.74 & 2.25 \\ 
  Cuz\`an & 0.00 & 1.27 & 0.95 \\ 
   \bottomrule
\end{tabular}
\end{center}
\end{table}


Figure~\ref{pres} shows the posterior predictive distribution for the
2008 election (top) and, based on the forecasts from each of the
component models, the 2012 election.  Component models
predictive distributions are shown in color (scaled by their
respective weight), while the EBMA predictive distribution is shown in
black. Vertical dashes indicate the point prediction of each model
(bold dash for the EBMA model). The vertical dashed line in the top
panel depicts the actual election result in 2008.

\begin{figure}[h]
\caption{Predictive ensemble PDFs of incumbent-part vote share in U.S. Presidential Elections}
\label{pres}
\begin{center}
\includegraphics[scale=.8]{presForecast}
\end{center}
The figure shows the density functions for each of the component
models in different colors and scaled by their respective weight. The
point predictions of the individual models are depicted by smalls
vertical dashes. The black curve is the density of the EBMA
prediction, with the bold dash indicating the EBMA point
prediction. For 2008 the vertical dashed line shows the actual result.
\end{figure}


FLORIAN: WE NEED TO UPDATE THIS TO SHOW THE ACTUAL RESULTS.  TRY TO
ADD A DISCUSSION HERE OF HOW GOOD THE METHOD IS.  I SUPPOSE WE SHOULD
RUN THE REGULAR EBMA WITHOUT C AND SEE HOW WE DO IN COMPARISON.  TRY
AND PUT SOME LIPSTICK ON THAT PIG, AND THEN WE'LL SEE WHERE WE ARE AT.



\section{Discussion} 

FLORIAN: CAN YOU ADD SOMETHING IN HERE ABOUT WHY WE DON'T ESTIMATE C AND HOW WE MIGHT DO THAT IN THE FUTURE?


Ensemble Bayesian model averaging is a principled way of combining
forecasts to improve prediction accuracy. However, the calibration of
such models in the social sciences is often hindered by the quality as
well as availability of data. For one, in many forecasting exercises
the number of forecasting models is large, yet the number of
observations on which the EBMA model can be trained is small. This
creates problems for the estimation of model weights, as it is likely
that overly high weights are assigned to models that are performing
well over this particular period. This is especially true should the
EBMA model be calibrated on in-sample forecasts of the component
models. Second, many predictive models do not provide forecasts for
all observations in the sample, as some forecasts may be missing or
the time-periods for which forecasts were made are different for
different models. In the standard EBMA model introduced in
\citet{mhw:2012} missing observations in component model predictions
are not allowed.

In this article, we address both of these issues to make EBMA more
applicable for researchers and predictioneers in the social
sciences. After reviewing the standard EBMA framework, we proceed
introduce a ``wisdom of the crowds'' parameter into the model, which
forces EBMA to put some minimal weight on all component models. Adding
this constant aids the calibration of EBMA when the number of
observations in the calibration period is small.

After explaining our adjustments, we illustrated its advantages via
simulation.  We then applied the adjusted EBMA model in two prediction
exercises. We use ensemble Bayesian model averaging to combine
predictions of the unemployment rate in the US from the Survey of
Professional Forecasters as well as the Green Book. As we show, even
when a large number of forecasts is missing for any given quarter,
EBMA generally outperforms the Green Book, SPF component models, as
well as the median and mean SPF forecast.

In a second example, we use the out-of-sample forecasts of nine
prediction models of presidential elections from 1992 to 2008 to
calibrate an ensemble model. We use the model calibrated to make an
informed prediction for the 2012 elections based on a weighted
combination of the component model predictions for 2012. This example
neatly illustrates the common difficulties facing forecasters in the
social science, and provides an illustrative example for applied
researchers going forward.

% Missing data is always a conundrum, and a pain. This is especially
% true when creating ensemble predictions. The combination of missing
% observations with short calibration periods is especially
% damaging. EBMA typically underweights the ensembles with a lot of
% missing observations, and as a result can diminish, rather than
% enhance, the predictive accuracy of the weighted average. We introduce
% a way around this problem by the introduction of a parameter which
% spreads the weights out over the ensemble components in a way that
% helps to preserve the advantages of the ensemble.

FLORIAN ... FUTURE WORK ?  MAYBE FOCUS ON BETTER WAYS OF HANDLING
MISSING DATA.


%%Bib 
\singlespacing
\bibliographystyle{apsr}
\bibliography{Bibliography_EBMA}


 \newpage
 \appendix

\doublespacing

 \section*{Appendix A: EM-Algorithm for missing data}

To accommodate missing values in component models prediction within
the EBMA procedure we follow \citet{Fraley:2010} and modify the EM
algorithm as follows.  Define $$\mathcal{A}^t = \{i|\mbox{ensemble
  member i available at time t}\},$$\noindent which is simply the
indicators of the list of components that provide forecasts for
observation $y^t$.  For convenience, define $\tilde{z}_k^{(j+1)t}
\equiv {{\underset{k \in
      A^t}{\sum}}\hat{w}^{(j)}_kp^{(j)}(y|f_{k}^{t})}/{\underset{k \in
    A^t}\sum w_k^{(j)}}$.  Equation \ref{E-step} above is then
replaced with

\begin{equation}
\hat{z}^{(j+1)t}_{k} = \Bigg\{ \begin{array}{c} {\hat{w}^{(j)}_k p^{(j)}(y|f_{k}^{t})}/{\tilde{z}_k^{(j+1)t} } \mbox{ if } k \in \mathcal{A}^t\\ 0 \mbox{ if } k \notin \mathcal{A}^t \end{array}
\end{equation}



\noindent  The M steps in Equations \ref{M-step} and \ref{sigma} are likewise replaced with

\begin{equation}
\hat{w}^{(j+1)}_k=\frac{\underset{t}{\sum}\hat{z}^{(j+1)t}_{k}}{\underset{t}{\sum}\overset{K}{\underset{k=1}{ \sum}} \hat{z}_k^{(j+1)t}}
\end{equation}


\noindent and

\begin{equation}
\hat{\sigma}^{2(j+1)}=\frac{\underset{t}{\sum}\overset{K}{\underset{k=1}{\sum}}\hat{z}^{(j+1)t}_{k}(y-f_{k}^{t})^2 }{\underset{t}{\sum}\overset{K}{\underset{k=1}{ \sum}} \hat{z}_k^{(j+1)t}}.
\end{equation}

\noindent In essence, the likelihood is renormalized given the missing
ensemble observations prior to maximization. Using the adjustments
above, the EBMA algorithm now allows for missing observations in the
component predictions.



 \section*{Appendix B: Predictive Metrics}

FLORIAN -- CAN YOU ADD THE MATHEMATICAL DETAILS OF CRPS HERE.  DONT GET CARRIED AWAY


 Denote the forecast of observation $i$ as $f_i$ and the observed
 outcome as $y_i$.  We define the \textit{absolute error} as
 $e_i\equiv |f_i - y_i|$ and the \textit{absolute percentage error} as
 $a_i \equiv e_i / |y_i| \times 100$.  Finally, for each observation we have
 prediction from naive forecast, $r_i$, that serves as a baseline for
 comparison.  In the example is the main text, this naive model is
 simply the lagged observation.  We can therefore define $b_i \equiv
 |r_i - y_i|$.\footnote{See \citet{brandt:freeman:schrodt:2011} for
   additional discussion of comparative fit metrics.}

 Donating the median of some vector $\mathbf{x}$ as $med(\mathbf{x})$,
 and the standard indicator function as $I(.)$, we define the following heuristic statistics:
 \begin{eqnarray*}
 \mathrm{MAE} &=& \frac{\sum_1^n{e_i}}{n}\\
  \mathrm{RMSE} &=& \sqrt{\frac{\sum_1^n{e^2_i}}{n}} \\
  \mathrm{MAD} &=& \mathrm{med}(\mathbf{e}) \\
  \mathrm{RMSLE} &=& \sqrt{\frac{\sum_1^n\left(ln(f_i+1) - ln(y_i+1)  \right)^2}{n}} \\
  \mathrm{MAPE} &=& \frac{\sum_1^n{a_i}}{n} \\
   \mathrm{MEAPE} &=& \mathrm{med}(\mathbf{a}) \\
 \mathrm{MRAE} &=& \mathrm{med}\left(\frac{e_1}{b_1}, \ldots, \frac{e_n}{b_n} \right) \\
 \mathrm{PW} &=& \frac{\sum_1^nI(e_i > b_i)}{n} \times 100
\end{eqnarray*}



\end{document}
\bye
